{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://epubs.siam.org/doi/10.1137/20M1382386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[0.5, 0.05, 0.1, 0.2], [0., 0.2, 0.3, 0.1], [0.06, 0.1, 0.2, 0.4], [0.05, 0.2, 0.15, 0.1]])\n",
    "B = torch.tensor([[-0.05, -0.01], [-0.005, -0.01], [-1, -0.01], [-0.01, -0.9]])\n",
    "Q_t = torch.tensor([[1., 0.5, -0.01, 0.], [-0.1, 1.1, 0.2, 0.], [0., 0.1, 0.9, -0.06], [0.03, 0., -0.1, 0.88]])\n",
    "R_t = torch.tensor([[0.4, -0.2], [-0.3, 0.7]])\n",
    "W = torch.diag(torch.tensor([0.1, 0.5, 0.2, 0.3]))\n",
    "# print(A, B, Q_t, R_t, W)\n",
    "Q_T = Q_t\n",
    "\n",
    "T = 10 # finite time horizon\n",
    "# r = 1 # smoothing parameter\n",
    "d = 4\n",
    "k = 2\n",
    "\n",
    "Q_t_block_diag = torch.block_diag(*[Q_t for _ in range(T)])\n",
    "R_t_block_diag = torch.block_diag(*[R_t for _ in range(T)])\n",
    "# Q_t_TXT = Q_t.repeat(T, T)\n",
    "# R_t_TXT = R_t.repeat(T, T)\n",
    "\n",
    "epochs = 1000\n",
    "num_samples = 25000 # expectation\n",
    "# print(Q_t_block_diag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_init_state(size:int) -> torch.tensor:\n",
    "    # x_0_1 = torch.normal(mean=torch.tensor([5.0]), std=torch.tensor([0.1**0.5]))  \n",
    "    # x_0_2 = torch.normal(mean=torch.tensor([2.0]), std=torch.tensor([0.3**0.5]))  \n",
    "    # x_0_3 = torch.normal(mean=torch.tensor([8.0]), std=torch.tensor([1.**0.5]))  \n",
    "    # x_0_4 = torch.normal(mean=torch.tensor([5.0]), std=torch.tensor([0.5**0.5]))\n",
    "    # mean = torch.tensor([5.0, 2.0, 8.0, 5.0])\n",
    "    size = (1, size)\n",
    "    x_0_1 = torch.normal(mean=5.0, std=0.1**0.5, size=size)  \n",
    "    x_0_2 = torch.normal(mean=2.0, std=0.3**0.5, size=size)  \n",
    "    x_0_3 = torch.normal(mean=8.0, std=1.**0.5, size=size)  \n",
    "    x_0_4 = torch.normal(mean=5.0, std=0.5**0.5, size=size)\n",
    "    \n",
    "    x_0 = torch.stack((x_0_1, x_0_2, x_0_3, x_0_4), dim=1)                                  \n",
    "    x_0 = torch.squeeze(x_0, dim=0)\n",
    "    # x_0 = torch.unsqueeze(x_0, 1)\n",
    "    # x_0 = x_0.to(device)|\n",
    "    return x_0\n",
    "\n",
    "print(gen_init_state(num_samples).shape)\n",
    "# print(gen_init_state(num_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noise $w_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.sqrt(torch.diag(W)).view(-1,1)\n",
    "print(w)\n",
    "w_noise = torch.randn((d, num_samples))\n",
    "w_stack = w * w_noise\n",
    "# print((w_stack**2).mean(dim=1))\n",
    "# print(w.shape)\n",
    "# w_stack = w.repeat(1, num_samples)\n",
    "# print(w_stack.shape)\n",
    "\n",
    "# L = torch.cholesky(W)\n",
    "# standard_normal_noise = torch.randn(L.size(0), num_samples)\n",
    "# w_stack = torch.matmul(L, standard_normal_noise)\n",
    "# print(w_stack.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network of Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ControlNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ControlNN, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(d, 100)\n",
    "        self.hidden_layer1 = nn.Linear(100, 100)\n",
    "        self.hidden_layer2 = nn.Linear(100, 100)\n",
    "        self.output_layer = nn.Linear(100, k*T)\n",
    "\n",
    "        # self.act = nn.ReLU()\n",
    "        # self.act1 = nn.Softplus()\n",
    "        # self.act2 = nn.Tanh()\n",
    "        # self.act = nn.CELU()\n",
    "        # self.act1 = nn.LeakyReLU()\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "        # self.relu = nn.ReLU()\n",
    "        # self.softplus = nn.Softplus()\n",
    "        # self.tanh = nn.Tanh()\n",
    "        # self.celu = nn.CELU()\n",
    "        # self.leakyrelu = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' \n",
    "        Input\n",
    "        shape (4, 100) = (d, num_samples)\n",
    "\n",
    "        Output\n",
    "        shape (20, 100) = (k*T, num_samples)\n",
    "        '''\n",
    "        x = x.permute(1, 0) # (100, 4)\n",
    "        # x = torch.squeeze(x, dim=1)\n",
    "\n",
    "        x = self.input_layer(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.act1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = self.hidden_layer1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.act2(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = self.hidden_layer2(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.act1(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        # x = self.act2(x)\n",
    "        x = self.act(x)\n",
    "\n",
    "        # x = torch.unsqueeze(x, dim=1)\n",
    "        x = x.permute(1, 0)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlNN = ControlNN()\n",
    "control= controlNN(gen_init_state(num_samples))\n",
    "print(control.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A.to(device)\n",
    "B = B.to(device)\n",
    "Q_t = Q_t.to(device)\n",
    "R_t = R_t.to(device)\n",
    "# W = W.to(device)\n",
    "Q_T = Q_T.to(device)\n",
    "# x_0 = x_0.to(device)\n",
    "# K_0 = K_0.to(device)\n",
    "# w_matrix = w_matrix.to(device)\n",
    "# state = state.to(device)\n",
    "# w = w.to(device)\n",
    "w_stack = w_stack.to(device)\n",
    "\n",
    "controlNN = controlNN.to(device)\n",
    "\n",
    "\n",
    "Q_t_block_diag = Q_t_block_diag.to(device)\n",
    "R_t_block_diag = R_t_block_diag.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(controlNN.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = gen_init_state(num_samples).to(device) # (4, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control = controlNN(x0) # (20, 100) (k*T, 100)\n",
    "# control = controlNN # (20, 100) (k*T, 100)\n",
    "# control.to(device)\n",
    "controlNN.train()\n",
    "\n",
    "# for _ in tqdm(range(epochs)):\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    optim.zero_grad()\n",
    "\n",
    "    control = controlNN(x0)\n",
    "    x = x0\n",
    "    state = torch.tensor([]).to(device)\n",
    "    state = torch.cat((state, x), dim=0)\n",
    "\n",
    "    for t in range(T-1):\n",
    "        u = control[t*k : (t+1)*k, :] # (2, 100)\n",
    "        x = torch.matmul(A, x) + torch.matmul(B, u) + w_stack # (4, 100)\n",
    "        # x = A @ x + B @ u + w_stack # (4, 100)\n",
    "        state = torch.cat((state, x), dim=0)\n",
    "    # state (40, 100) (d*T, 100) / control (20, 100) (k*T, 100)\n",
    "    u = control[(T-1)*k : T*k, :]\n",
    "    # x_terminal = torch.matmul(A, x) + torch.matmul(B, u) + w_stack # Update terminal state 'x_T' \n",
    "    x_terminal = A @ x + B @ u + w_stack # Update terminal state 'x_T' \n",
    "    \n",
    "    # loss_state = torch.matmul(torch.transpose(state, 0, 1), torch.matmul(Q_t_block_diag, state))\n",
    "    loss_state = state.T @ Q_t_block_diag @ state\n",
    "    loss_state_diag = loss_state.diagonal()\n",
    "    # print(loss_state_diag.shape)\n",
    "    avg_loss_state = loss_state_diag.mean() \n",
    "    # print(avg_loss_state.shape)\n",
    "\n",
    "    # loss_control = torch.matmul(torch.transpose(control, 0, 1), torch.matmul(R_t_block_diag, control)) \n",
    "    loss_control = control.T @ R_t_block_diag @ control\n",
    "    # print(loss_control.shape)\n",
    "    loss_control_diag = loss_control.diagonal()\n",
    "    avg_loss_control = loss_control_diag.mean() \n",
    "\n",
    "    # print('x_T', x_terminal.shape) # (4, 100)\n",
    "    # print('Q_t', Q_t.shape) # (4, 4)\n",
    "    # loss_terminal_state = torch.matmul(torch.transpose(x_terminal, 0, 1), torch.matmul(Q_t, x_terminal))\n",
    "    loss_terminal_state = x_terminal.T @ Q_t @ x_terminal\n",
    "    loss_terminal_state_diag = loss_terminal_state.diagonal()\n",
    "    avg_loss_terminal_state = loss_terminal_state_diag.mean()\n",
    "\n",
    "    loss = avg_loss_state + avg_loss_control + avg_loss_terminal_state\n",
    "    if epoch % 5000 == 0:\n",
    "        torch.save(controlNN.state_dict(), 'LQR.pt')\n",
    "        # torch.save(controlNN.state_dict(), f'LQR_{str(controlNN.act2)}.pt')\n",
    "        print(f'epoch: {epoch}, objective: {loss.item()}')\n",
    "    # print(loss)\n",
    "    # print('bboa?')\n",
    "    loss.backward()\n",
    "    # print('back!!!')\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print('shape of control:', control.shape)\n",
    "print('shape of state:', state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real Solution of LQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sol_P(A, B, Q_t, Q_T, R_t, device, num_steps=1000, tol=1e-6):\n",
    "def get_sol_P():\n",
    "    P_list = []\n",
    "    P_next = Q_T # Terminal condition\n",
    "    P_list.append(P_next)\n",
    "\n",
    "    for _ in range(T): # P_T^*, P_{T-1}^*, ..., P_0^*\n",
    "        P_t = Q_t + A.T @ P_next @ A \n",
    "        - A.T @ P_next @ B @ torch.linalg.inv(B.T @ P_next @ B + R_t) @ B.T @ P_next @ A\n",
    "        P_list.append(P_t)\n",
    "        P_next = P_t\n",
    "    \n",
    "    P_list.reverse() # P_0^*, P_1^*, ..., P_T^*\n",
    "    \n",
    "    return P_list\n",
    "\n",
    "P_sol = get_sol_P()\n",
    "# len(P_sol)\n",
    "# P_sol[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sol_K(P):\n",
    "    P_sol_list = P\n",
    "    P_sol_list.reverse() # P_T^*, P_{T-1}^*, ..., P_0^*\n",
    "    K_list = []\n",
    "    \n",
    "    for i in range(T): # K_{T-1}^*, ..., K_0^*\n",
    "        K_t = torch.linalg.inv(B.T @ P_sol_list[i] @ B + R_t) @ B.T @ P_sol_list[i] @ A\n",
    "        K_list.append(K_t)\n",
    "\n",
    "    K_list.reverse() # K_0^*, K_1^*, ..., K_{T-1}^*\n",
    "\n",
    "    return K_list\n",
    "\n",
    "K_sol = get_sol_K(P_sol)\n",
    "# len(K_sol)\n",
    "# print(K_sol[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sol_u(K, x_0):\n",
    "    x_sol = torch.zeros_like(state).to(device)\n",
    "    u_sol = torch.zeros_like(control).to(device)\n",
    "\n",
    "    x_sol[0*d : 1*d, :] = x_0\n",
    "    \n",
    "    for i in range(T-1):\n",
    "        K_t = K[i]\n",
    "        # x_t = x[i*d : (i+1)*d, :] # [x_i^0, ..., x_i^{num_samples - 1}]\n",
    "        x_t = x_sol[i*d : (i+1)*d, :]\n",
    "        u_sol[i*k : (i+1)*k, :] = - K_t @ x_t # Update u_t\n",
    "\n",
    "        u_t = u_sol[i*k : (i+1)*k, :]\n",
    "        x_sol[(i+1)*d : (i+2)*d, :] = A @ x_t + B @ u_t + w_stack # Update x_t\n",
    "\n",
    "    K_t = K[T-1]\n",
    "    x_t = x_sol[(T-1)*d : T*d, :]\n",
    "    u_sol[(T-1)*k : T*k, :] = - K_t @ x_t  # Update u_{T-1}\n",
    "    # print('xsol', x_sol)\n",
    "    return u_sol\n",
    "    \n",
    "u_sol = get_sol_u(K_sol, x0)\n",
    "# print(u_sol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.norm(control-u_sol, p=2)/num_samples\n",
    "# torch.norm(u_pred - u_sol, p='fro')/num_samples\n",
    "# loss_tr = loss_fn(u_pred, u_sol)\n",
    "\n",
    "# print(f'Test loss: {loss_test.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlNN.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_test = gen_init_state(num_samples).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    u_pred_test = controlNN(x0_test)\n",
    "# print(u_pred_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_sol_test = get_sol_P()\n",
    "K_sol_test = get_sol_K(P_sol_test)\n",
    "u_sol_test = get_sol_u(K_sol_test, x0_test)\n",
    "\n",
    "# print(u_sol_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.norm(u_pred_test - u_sol_test, p='fro')/num_samples\n",
    "loss_test = loss_fn(u_pred_test, u_sol_test)\n",
    "\n",
    "print(f'Test loss: {loss_test.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LQR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
