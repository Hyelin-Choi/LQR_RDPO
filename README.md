# Solving Linear Quadratic Regulator (LQR) with **Regularized Direct Policy Optimization (RDPO)**

## 1. Problem Setup

* **Goal**: Solve the finite-horizon **Linear Quadratic Regulator (LQR) problem** using **Regularized Direct Policy Optimization (RDPO)** and compare the learned control sequence with the exact optimal solution from the Riccati equation.
* **System dynamics**:

  $$x_{t+1} = A x_t + B u_t + w_t$$

  where $w_t$ is Gaussian noise, and $x_0$ is drawn from a given distribution.
* **Cost function**:

  $$J = \mathbb{E} \left[ \sum_{t=0}^{T-1} \left( x_t^\top Q_t x_t + u_t^\top R_t u_t \right) + x_T^\top Q_T x_T \right]$$
  $$J = \mathbb{E} \left[ \sum_{t=0}^{T-1} \left( x_t^\top Q_t x_t + u_t^\top R_t u_t \right) + x_T^\top Q_T x_T \right]$$
* **Dimensions**: State $d = 4$, control $k = 2$, horizon $T = 10$.

---

## 2. Policy Network (ControlNN)

* **Input**: Initial state $x_0 \in \mathbb{R}^d$
* **Output**: Control sequence $(u_0, \dots, u_{T-1}) \in \mathbb{R}^{kT}$
* **Architecture**:

  ```
  Linear(d â†’ 100) â†’ Tanh
  Linear(100 â†’ 100) â†’ Tanh
  Linear(100 â†’ 100) â†’ Tanh
  Linear(100 â†’ kT)
  ```
* **Type**: Open-loop policy â€” generates the full sequence of control inputs in one forward pass.

---

## 3. RDPO Training Procedure

1. **Sample initial states** $x_0^{(i)}$, $i = 1, \dots, N$.
2. **Generate control sequences** $u_t^{(i)}$ using the policy network.
3. **Simulate system dynamics** using $A, B$ to produce trajectories.
4. **Compute the RDPO loss**:

   * **State cost**:

     $$\frac{1}{N} \sum_{i=1}^N \sum_{t=0}^{T-1} x_t^\top Q_t x_t$$
   * **Control cost**:

     $$\frac{1}{N} \sum_{i=1}^N \sum_{t=0}^{T-1} u_t^\top R_t u_t$$
   * **Terminal state cost**:

     $$\frac{1}{N} \sum_{i=1}^N x_T^\top Q_T x_T$$
   * **Total RDPO loss** = state + control + terminal costs
     (Optionally includes regularization terms depending on RDPO variant)
5. **Backpropagation** through time (BPTT) to update policy parameters.
6. **Optimizer**: Adam (learning rate = $1\times 10^{-4}$).

ðŸ“Œ **Nature of RDPO**:

* Model-based direct policy optimization
* Uses differentiable dynamics and exact gradient computation
* Regularization can improve stability and generalization
* No value function or Q-function approximation

---

## 4. Computing the Exact LQR Solution

* Solve the **backward Riccati equation**:

  $$P_t^\* = Q_t + A^\top P_{t+1}^\* A - A^\top P_{t+1}^\* B (B^\top P_{t+1}^\* B + R_t)^{-1} B^\top P_{t+1}^\* A$$
* **Optimal feedback gain**:

  $$K_t^\* = (B^\top P_{t+1}^\* B + R_t)^{-1} B^\top P_{t+1}^\* A$$
* Compute the **optimal control sequence**:

  $$u_t^\* = -K_t^\* x_t$$

---

## 5. Evaluation

* **u\_pred**: Control sequence generated by ControlNN (RDPO-trained).
* **u\_sol**: Optimal sequence from the Riccati solution.
* **Metric**: Mean Squared Error (MSE) between $u_{\text{pred}}$ and $u_{\text{sol}}$.


